{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of neural networks\n",
    "\n",
    "### Making neural nets from scratch in Python\n",
    "\n",
    "The following is meant to be an entrance level tutorial on programming neural networks from scratch in Python 3 for those interested while at the same time serving as my personal notes.\n",
    "\n",
    "In theory anyone could follow along but if the wish of the reader is to fully understand whatâ€™s going on, I recommend a preliminary understanding of neural nets (like the different layers and their relation) along with some coding knowledge (in particular data structures) and basic math. \n",
    "\n",
    "### Example 1: Basic Neural Network (Neurons: 3 input, 8 hidden, 1 output)\n",
    "\n",
    "An experienced coder might disapprove of the variable naming in the first example, however this is done in order to clarify each elements particular purpose and the formalization will be simplified in due time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports and methods\n",
    "import numpy as np\n",
    "\n",
    "def derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current error margin: 0.34711792424839616\n",
      "Current error margin: 0.0009092139123413293\n",
      "Current error margin: 0.000636275056311007\n",
      "Current error margin: 0.0005176358625924139\n",
      "Current error margin: 0.00044737320821574807\n",
      "Current error margin: 0.0003995719142249806\n",
      "Current error margin: 0.00036434682089044783\n",
      "Current error margin: 0.0003369983937423288\n",
      "Current error margin: 0.00031496844415966813\n",
      "Current error margin: 0.0002967292920532618\n"
     ]
    }
   ],
   "source": [
    "# Simple Neural Network (Neurons: 3 input, 8 hidden, 1 output) training\n",
    "\n",
    "input_neurons = np.array([[0, 0, 0],\n",
    "                          [0, 0, 1],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 1],\n",
    "                          [1, 0, 0],\n",
    "                          [1, 0, 1],\n",
    "                          [1, 1, 0],\n",
    "                          [1, 1, 1]])\n",
    "\n",
    "output_neurons = np.array([[0],\n",
    "                           [1],\n",
    "                           [1],\n",
    "                           [0],\n",
    "                           [1],\n",
    "                           [1],\n",
    "                           [1],\n",
    "                           [1]])\n",
    "\n",
    "\n",
    "# Generating random link values (and normalizing)\n",
    "synapse_InputToHidden = np.random.random((3, 8)) * 2 - 1\n",
    "synapse_HiddenToOutput = np.random.random((8, 1)) * 2 - 1\n",
    "\n",
    "\n",
    "for i in range(100000):  # Running 100,000 times to decrease error margin thereby optimizing the network\n",
    "    \n",
    "    input_layer = input_neurons\n",
    "    hidden_layer = sigmoid(np.dot(input_layer, synapse_InputToHidden))\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, synapse_HiddenToOutput))\n",
    "    \n",
    "    # Gradient descent (more specifically a \"backward propagation of errors\" - Backpropagation)\n",
    "    errormargin_OutputLayer = output_neurons - output_layer\n",
    "    delta_OutputLayer = errormargin_OutputLayer * derivative(output_layer)\n",
    "    errormargin_HiddenLayer = delta_OutputLayer.dot(synapse_HiddenToOutput.T)\n",
    "    delta_HiddenLayer = errormargin_HiddenLayer * derivative(hidden_layer)\n",
    "    \n",
    "    synapse_InputToHidden += input_layer.T.dot(delta_HiddenLayer)\n",
    "    synapse_HiddenToOutput += hidden_layer.T.dot(delta_OutputLayer)\n",
    "    \n",
    "    # Prints error margin for output layer every 10,000 iderations\n",
    "    if i % 10000 == 0:\n",
    "        print(\"Current error margin: {}\".format(str(np.mean(abs(errormargin_OutputLayer)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notice how our error margin continuously deceases optimizing our neural network)\n",
    "\n",
    "The final output layer is printed as well as our trained synapses in order to get a feel for what our network looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00482612]\n",
      " [0.99621326]\n",
      " [0.9956539 ]\n",
      " [0.00403061]\n",
      " [0.99999017]\n",
      " [0.99914111]\n",
      " [0.99856388]\n",
      " [0.9993305 ]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the final output layer\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.47758845  0.33370591 -1.45289829  0.31914448 -0.33103278  1.04997553\n",
      "   4.40350748 -3.49052029]\n",
      " [-5.47377066 -0.45858454  5.37705705  4.28999617 -0.54616914 -0.15824309\n",
      "   3.78541502  6.11554177]\n",
      " [-5.34674264 -0.01653795 -2.47468438  4.06672141  0.87626689 -0.09174594\n",
      "  -7.47609678 -2.93473276]]\n",
      "[[-15.74131092]\n",
      " [ -0.20007949]\n",
      " [ -5.35498673]\n",
      " [  7.41193567]\n",
      " [ -1.32794695]\n",
      " [  0.41568824]\n",
      " [ 11.91534665]\n",
      " [ -7.77640579]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the weightings our two synapses\n",
    "print(synapse_InputToHidden)\n",
    "print(synapse_HiddenToOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens in we send a new input through our neural net (output should be 0, since the new input corresponds to the first input in our training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0048261]\n"
     ]
    }
   ],
   "source": [
    "new_input = np.array([0, 0, 0])\n",
    "\n",
    "new_output = sigmoid(np.dot(sigmoid(np.dot(new_input, synapse_InputToHidden)), synapse_HiddenToOutput))\n",
    "\n",
    "print(new_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close! It it worth mentioning that it is possible to mess up the structure of the synapses (e.g. try training the system with output_neurons = 0, 0, 0, 0, 1, 1, 1, 1. The reason for this problem will be investigated later on.\n",
    "\n",
    "### Example 2: Deep Neural Network (Neurons: 4 input, 8 hidden, 16 hidden, 8 hidden, 2 output)\n",
    "\n",
    "We keep Example 1's methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current error margin: 0.44711071583178513\n",
      "Current error margin: 0.004957724608214143\n",
      "Current error margin: 0.003368736657040206\n",
      "Current error margin: 0.002699241313895848\n",
      "Current error margin: 0.0023102055028723827\n",
      "Current error margin: 0.00204909238051682\n",
      "Current error margin: 0.0018586619092116137\n",
      "Current error margin: 0.00171203849427689\n",
      "Current error margin: 0.001594739285055455\n",
      "Current error margin: 0.0014981884017274266\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Network (Neurons: 4 input, 8 hidden, 16 hidden, 8 hidden, 2 output)\n",
    "\n",
    "input_neurons = np.array([[0, 0, 0, 0],\n",
    "                          [0, 0, 0, 1],\n",
    "                          [0, 0, 1, 0],\n",
    "                          [0, 0, 1, 1],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [0, 1, 0, 1],\n",
    "                          [0, 1, 1, 0],\n",
    "                          [0, 1, 1, 1]])\n",
    "\n",
    "output_neurons = np.array([[1, 0],\n",
    "                           [1, 1],\n",
    "                           [1, 1],\n",
    "                           [1, 0],\n",
    "                           [1, 1],\n",
    "                           [0, 1],\n",
    "                           [0, 0],\n",
    "                           [1, 1]])\n",
    "\n",
    "# Deterministic generation such that we can better compare different network\n",
    "np.random.seed(1)\n",
    "\n",
    "# Generating and normalizing synapses\n",
    "synapse_1 = np.random.random((4, 8)) * 2 - 1\n",
    "synapse_2 = np.random.random((8, 16)) * 2 - 1\n",
    "synapse_3 = np.random.random((16, 8)) * 2 - 1\n",
    "synapse_4 = np.random.random((8, 2)) * 2 - 1\n",
    "\n",
    "\n",
    "for i in range(100000):  # Training\n",
    "\n",
    "    layer_1 = input_neurons\n",
    "    layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "    layer_3 = sigmoid(np.dot(layer_2, synapse_2))\n",
    "    layer_4 = sigmoid(np.dot(layer_3, synapse_3))\n",
    "    layer_5 = sigmoid(np.dot(layer_4, synapse_4))\n",
    "\n",
    "    # Backpropagation\n",
    "    layer_5_error = output_neurons - layer_5\n",
    "    layer_5_delta = layer_5_error * derivative(layer_5)\n",
    "    layer_4_error = layer_5_delta.dot(synapse_4.T)\n",
    "    layer_4_delta = layer_4_error * derivative(layer_4)\n",
    "    layer_3_error = layer_4_delta.dot(synapse_3.T)\n",
    "    layer_3_delta = layer_3_error * derivative(layer_3)\n",
    "    layer_2_error = layer_3_delta.dot(synapse_2.T)\n",
    "    layer_2_delta = layer_2_error * derivative(layer_2)\n",
    "\n",
    "    synapse_1 += layer_1.T.dot(layer_2_delta)\n",
    "    synapse_2 += layer_2.T.dot(layer_3_delta)\n",
    "    synapse_3 += layer_3.T.dot(layer_4_delta)\n",
    "    synapse_4 += layer_4.T.dot(layer_5_delta)\n",
    "\n",
    "\n",
    "    # Prints error margin\n",
    "    if i % 10000 == 0:\n",
    "        print(\"Current error margin: {}\".format(str(np.mean(abs(layer_5_error)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99836656 0.00211058]\n",
      " [0.99934177 0.99944946]\n",
      " [0.99877575 0.99800481]\n",
      " [0.99884066 0.00190899]\n",
      " [0.99845804 0.99853785]\n",
      " [0.00174104 0.99999543]\n",
      " [0.0023237  0.00171322]\n",
      " [0.99886795 0.99848799]]\n"
     ]
    }
   ],
   "source": [
    "print(layer_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the network is modeled pretty accurately!\n",
    "\n",
    "We can tweak the parameters of the synapses thereby creating different networks. Subsequently we can compare their error rates thus optimizing the networks structure for this particular task. Here are 3 final error margins of 3 different networks:\n",
    "\n",
    "(Neurons: 4 input, 8 hidden, 16 hidden, 8 hidden, 2 output): 0.00020308381653301406\n",
    "(Neurons: 4 input, 8 hidden, 8 hidden, 8 hidden, 2 output): 0.0002564333501877733\n",
    "(Neurons: 4 input, 16 hidden, 8 hidden, 8 hidden, 2 output): 0.00010261191080388188\n",
    "\n",
    "Interestingly although a greater amount of hidden neurons in general seems better placing them in the beginning seems to give even better predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
