{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of neural networks 2 (OOP)\n",
    "\n",
    "### Making neural nets from scratch in Python using object-oriented programming\n",
    "\n",
    "This notebook is a follow up to my NN Basics 1. This time object-oriented is used thereby easier allowing for better re-usability and net structure management. The purpose of this project was also the build neural nets without the use of NumPy, thereby gaining deeper insight into the workings of neural nets. My hope is that these will serve as my future sketches for creating NN and as such a multitude of methods will be included and possibly more will be added along the way. This means that implementing this networks in practice we should probably refrain from including all of the available methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Node:\n",
    "\n",
    "A Neuron Node including verious activation functions. \n",
    "\n",
    "### Currently supported activation functions:\n",
    "<ul>\n",
    "  <li>Rectified Linear Unit (ReLU)</li>\n",
    "  <li>Step</li>\n",
    "  <li>Smooth Rectified Linear Unit (smoothReLU)</li>\n",
    "  <li>Sigmoid</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, no, bias=0):\n",
    "        self.no = no\n",
    "        self.connections = []\n",
    "        self.value = 0  # accumulated inputs (prior to activation)\n",
    "        self.bias = bias\n",
    "        self.activated_value = 0  # value post activation function\n",
    "        self.enabled = True\n",
    "\n",
    "    def activation(self, activation_type):\n",
    "        self.value += self.bias\n",
    "        \n",
    "        if activation_type == 'ReLU':\n",
    "            self.ReLU_activation()\n",
    "            \n",
    "        elif activation_type == 'sigmoid':\n",
    "            self.sigmoid_activation()\n",
    "            \n",
    "        elif activation_type == \"smoothReLU\":\n",
    "            self.smoothReLU_activation()\n",
    "            \n",
    "        elif activation_type == 'step':\n",
    "            self.step_activation()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Doesn't support this type of activation.\")\n",
    "    \n",
    "    def ReLU_activation(self):\n",
    "        self.activated_value = max(0, self.value)\n",
    "    \n",
    "    def step_activation(self):\n",
    "        self.activated_value = 1 if self.value > 0 else 0\n",
    "        \n",
    "    def smoothReLU_activation(self):\n",
    "        self.activated_value = math.log(1 + math.exp(self.value))\n",
    "\n",
    "    def sigmoid_activation(self):\n",
    "        self.activated_value = 1 / (1 + math.exp(-self.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synapse Node:\n",
    "\n",
    "A Synapse Node with the possibility of being enabled/disabled.\n",
    "\n",
    "### Currently supported methods:\n",
    "<ul>\n",
    "  <li>Enable synapse</li>\n",
    "  <li>Disable synapse</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synapse:\n",
    "    def __init__(self, from_neuron, to_neuron, weight=random.uniform(-2, 2)):\n",
    "        self.from_neuron = from_neuron\n",
    "        self.to_neuron = to_neuron\n",
    "        self.weight = weight\n",
    "        self.enable = True\n",
    "    \n",
    "    def enable(self):\n",
    "        self.enable = True\n",
    "        \n",
    "    def disable(self):\n",
    "        self.enable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network:\n",
    "\n",
    "Our NeuralNet class allows us to initialize essentially an empty network and it's methods allows us to add different kinds of connections between those layers (like in TensorFlow - see next chapter). Furthermore it can be trained in different ways. I hope to include more possibilities going forward.\n",
    "\n",
    "### Possible methods:\n",
    "<ul>\n",
    "  <li>Create input layer</li>\n",
    "  <li>Create output layer</li>\n",
    "  <li>Create hidden layer</li>\n",
    "  <li>Create dense connetion between two layers</li>\n",
    "</ul>\n",
    "\n",
    "### Possible training:\n",
    "<ul>\n",
    "  <li>Backpropagation</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.hidden = []  # This is a 2d list to allow for multiple hidden layers\n",
    "\n",
    "        self.layers = []  # Neurons\n",
    "        self.activations = []  # Activation function for the given layer\n",
    "        self.synapses = []  # Synapses\n",
    "\n",
    "    def add_layer(self, size, layer_type, connection='dense', activation='sigmoid', bias=0):\n",
    "\n",
    "        # If we are creating the input layer\n",
    "        if layer_type == 'input':\n",
    "            self.inputs = [Neuron(i, bias) for i in range(size)]\n",
    "            self.layers.append(self.inputs)\n",
    "            self.activations.append(activation)\n",
    "\n",
    "        # If we are creating the output layer\n",
    "        elif layer_type == 'output':\n",
    "            self.outputs = [Neuron(i, bias) for i in range(size)]\n",
    "\n",
    "            if connection == 'dense':\n",
    "                dense_connection = []\n",
    "\n",
    "                for from_neuron in self.layers[-1]:\n",
    "                    for to_neuron in self.outputs:\n",
    "                        synapse = Synapse(from_neuron, to_neuron, random.uniform(-2, 2))\n",
    "                        dense_connection.append(synapse)\n",
    "\n",
    "                self.synapses.append(dense_connection)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Output layer doesn't support that connection type!\")\n",
    "\n",
    "            self.layers.append(self.outputs)\n",
    "            self.activations.append(activation)\n",
    "\n",
    "        # If we are creating a hidden layer\n",
    "        elif layer_type == 'hidden':\n",
    "            hidden_layer = [Neuron(i, bias) for i in range(size)]\n",
    "            self.hidden.append(hidden_layer)\n",
    "\n",
    "            if connection == 'dense':\n",
    "                dense_connection = []\n",
    "\n",
    "                for from_neuron in self.layers[-1]:\n",
    "                    for to_neuron in self.hidden[-1]:\n",
    "                        synapse = Synapse(from_neuron, to_neuron, random.uniform(-2, 2))\n",
    "                        dense_connection.append(synapse)\n",
    "\n",
    "                self.synapses.append(dense_connection)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"This layer doesn't support that connection type!\")\n",
    "\n",
    "            self.layers.append(hidden_layer)\n",
    "            self.activations.append(activation)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Network doesn't support that layer type!\")\n",
    "\n",
    "    def train(self, input_values_index, target_values_index, iterations=10000, optimizing='backpropagation'):\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "            for index, input_values in enumerate(input_values_index):\n",
    "\n",
    "                # Network run\n",
    "                self.run(input_values)\n",
    "\n",
    "                # backpropagation as an optimizer\n",
    "                if optimizing == 'backpropagation':\n",
    "                    self.backpropagation(index, target_values_index)\n",
    "\n",
    "    def backpropagation(self, current_index, target_value_index):\n",
    "\n",
    "        current_targets = target_value_index[current_index]\n",
    "\n",
    "        for layer_index in range(len(self.layers) - 2, 0, -1):\n",
    "\n",
    "            layer_error_margin = [current_targets[i] - self.layers[layer_index + 1][i].activated_value for i in\n",
    "                                  range(len(self.layers[layer_index + 1]))]\n",
    "\n",
    "            layer_derivative = [neuron.activated_value * (1 - neuron.activated_value) for neuron in\n",
    "                                self.layers[layer_index + 1]]  # Derivative of the sigmoid\n",
    "\n",
    "            current_targets = [0 for _ in range(len(self.layers[layer_index]))]\n",
    "            for i, target in enumerate(current_targets):\n",
    "                for synapse in self.synapses[layer_index]:\n",
    "                    if synapse.from_neuron.no == i:\n",
    "                        target += synapse.weight * 2 * layer_error_margin[synapse.to_neuron.no] * layer_derivative[\n",
    "                            synapse.to_neuron.no]\n",
    "\n",
    "            # This complicated step requries some explaination:\n",
    "            # We are interested in finding the individual synapses' proportional influences on the cost (layer_error_margin**2) of the given layer\n",
    "            # In other words we want to find the derivative of the layers cost with respect to the individual synapses leading up the that layer\n",
    "            # This derivative is equal to the derivative of the cost to the activated layer (2 * layer_error_margin)\n",
    "            # Times the derivative of the activated layer to the unactivated layer (val * (1 - val) --- derivative of the sigmoid, which we did above)\n",
    "            # Times the derivative of the unactivated layer to synapses' influences (self.layers[layer_num] --- the previously activated layer)\n",
    "            # We thus want to update our synapses with: 2 * layer_error_margin * layer_derivative * self.layers[layer_num]\n",
    "            # Of course we only want to update the synapses based on its relevant connections, thus:\n",
    "            for synapse in self.synapses[layer_index]:\n",
    "                synapse.weight += 2 * layer_error_margin[synapse.to_neuron.no] * layer_derivative[\n",
    "                    synapse.to_neuron.no] * synapse.from_neuron.activated_value\n",
    "\n",
    "    def run(self, input_values):\n",
    "\n",
    "        # Reset network\n",
    "        self.reset()\n",
    "\n",
    "        # Check if our training data is normalized (between 0 and 1) and if it's not we run a sigmoid on our input layer\n",
    "        normalized_data = True\n",
    "\n",
    "        for i, val in enumerate(input_values):\n",
    "            if 0 >= val >= 1:\n",
    "                normalized_data = False\n",
    "\n",
    "            self.inputs[i].value = val\n",
    "            self.inputs[i].activated_value = val\n",
    "\n",
    "        if not normalized_data:\n",
    "            for neuron in self.inputs:\n",
    "                neuron.sigmoid_activation()\n",
    "\n",
    "        # For every layer in the network:\n",
    "        # We perform a multiplication between the neuron values in the current layer and the weights of the connections to the next layer\n",
    "        # Activate the specified activation function for the next layer\n",
    "        for layer_index in range(len(self.layers) - 1):\n",
    "\n",
    "            for synapse in self.synapses[layer_index]:\n",
    "                synapse.to_neuron.value += synapse.weight * synapse.from_neuron.activated_value\n",
    "\n",
    "            for neuron in self.layers[layer_index + 1]:\n",
    "                neuron.activation(self.activations[layer_index + 1])\n",
    "\n",
    "    def reset(self):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer:\n",
    "                neuron.value = 0\n",
    "                neuron.activated_value = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's instanciate a network and add some layers to our model. By default the connections are dense and they use a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNet()\n",
    "network.add_layer(3, 'input')\n",
    "network.add_layer(8, 'hidden')\n",
    "network.add_layer(16, 'hidden')\n",
    "network.add_layer(8, 'hidden')\n",
    "network.add_layer(1, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5678350077347651\n"
     ]
    }
   ],
   "source": [
    "network.run([1, 0, 0])\n",
    "print(network.outputs[0].activated_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward feeding the network with some input and printing the output neurons activated value, we can see that our network works as intended as our synapses are randomly generated.\n",
    "\n",
    "Let's train our network with our previous input and an expected output of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[1, 0, 0]]\n",
    "\n",
    "training_labels = [[0]]\n",
    "\n",
    "network.train(training_data, training_labels, iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004945296747181074\n"
     ]
    }
   ],
   "source": [
    "network.run([1, 0, 0])\n",
    "print(network.outputs[0].activated_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now how the synapses has gravitated the network toward an output of 0 and when we again feed our input, we get a pretty acturate output.\n",
    "\n",
    "We can futher train the network train the network in the other direction if we so disire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[1, 0, 0]]\n",
    "\n",
    "training_labels = [[1]]\n",
    "\n",
    "network.train(training_data, training_labels, iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950111194658214\n"
     ]
    }
   ],
   "source": [
    "network.run([1, 0, 0])\n",
    "print(network.outputs[0].activated_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might initially look very promising, however the way we have trained this network only makes is gravitate towards a certain output i.e. regardless of input the output will always be near 0.\n",
    "\n",
    "Weather this is due to some error or misunderstanding on my part regarding backpropagation or it occurs as a result of not yet having implemented a stochastic gradient decent I am not sure. But investigation will follow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
